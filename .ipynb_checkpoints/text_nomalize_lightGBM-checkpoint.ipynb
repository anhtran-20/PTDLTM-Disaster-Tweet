{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install và import thư viện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\tai_phan_mem\\python\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\tai_phan_mem\\python\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in d:\\tai_phan_mem\\python\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\tai_phan_mem\\python\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in d:\\tai_phan_mem\\python\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in d:\\tai_phan_mem\\python\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in d:\\tai_phan_mem\\python\\lib\\site-packages (0.7.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "zEEJMzHLP9u0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import html\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Đọc lại dữ liệu (Phần này ở tiền xử lý và khám phá không tác động nên khi merge file không cần cũng được)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "aYOEK1feTunq"
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"datasets/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "5Z4C9UnhT0cJ",
    "outputId": "97d17452-1251-43ed-e200-b6026ac3beaa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "gQUbm6ZzUMKB"
   },
   "outputs": [],
   "source": [
    "test_df=pd.read_csv(\"datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "C_RehUjvUSaO"
   },
   "outputs": [],
   "source": [
    "tweet_df=pd.concat([train_df,test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSD1UUFWUaEp",
    "outputId": "f643b38a-ea52-47a7-f3b6-2bfde7fb467f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 5)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "ReKLxOGLUkQD",
    "outputId": "238c5313-13e4-4b71-d0ff-6ca432b72f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  \n",
       "5     1.0  \n",
       "6     1.0  \n",
       "7     1.0  \n",
       "8     1.0  \n",
       "9     1.0  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy DataFrame gốc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lightGBM_df=train_df.__deepcopy__()\n",
    "test_lightGBM_df=test_df.__deepcopy__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nhận xét: \n",
    "    - Trong thuộc tính `text` của DataFrame ta thấy xuất hiện các thẻ của html nhưng chưa định dạng về đúng, ví dụ: &amp ~ &, &lt ~ <, &gt ~ >\n",
    "\n",
    "Chuẩn hóa lại `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_html(text):\n",
    "    return html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lightGBM_df.text=train_lightGBM_df.text.apply(decode_html)\n",
    "test_lightGBM_df.text=test_lightGBM_df.text.apply(decode_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo các list và dictionary để tiện cho việc xử dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticon thường sử dụng\n",
    "emoticons=['-.-', '-_-', '8)', '8-)', '8-D', '8D', ':!]', ':#', ':$', ':&', ':(', ':)', ':*', ':+', ':-(', ':-)', ':-*', ':-/', ':->', ':-?', ':-D', ':-O', ':-P', ':-]', ':-|', ':-}', ':/', ':3', ':>', ':@', ':D', ':O', ':P', ':]', ':^)', ':c)', ':o)', ':s', ':v', ':|', ':}', ';)', ';-)', '>:(', '<3', '=)', '=3', '=D', '=]', 'B^D', 'C:', 'O.o', '^_^', 'c:', 'o.o', \"):\", '8:' ,']:']\n",
    "\n",
    "# Các ký tự viết tắt\n",
    "truncated_words_dict={\"lol\" : \"laughing out loud\",\"brb\" : \"be right back\",\"btw\" : \"by the way\",\"omg\" : \"oh my god\",\"bff\" : \"best friend forever\",\"imo\" : \"in my opinion\",\"irl\" : \"in real life\",\"lmk\" : \"let me know\",\"np\" : \"no problem\",\"nvm\" : \"nevermind\",\"pls\" : \"please\",\"thx\" : \"thanks\",\"u\" : \"you\",\"ur\" : \"your\",\"wtf\" : \"what the fuck\",\"yolo\" : \"you only live once\",\"tbh\" : \"to be honest\",\"smh\" : \"shaking my head\",\"rn\" : \"right now\",\"fyi\" : \"for your information\",\"afk\" : \"away from keyboard\",\"bae\" : \"before anyone else\",\"bc\" : \"because\",\"cuz\" : \"because\",\"def\" : \"definitely\",\"dunno\" : \"do not know\",\"gonna\" : \"going to\",\"gr8\" : \"great\",\"idk\" : \"I do not know\",\"ikr\" : \"I know, right?\",\"jk\" : \"just kidding\",\"k\" : \"okay\",\"obv\" : \"obviously\",\"probs\" : \"probably\",\"rly\" : \"really\",\"sry\" : \"sorry\",\"ttyl\" : \"talk to you later\",\"yw\" : \"you are welcome\",\"ig\" : \"Instagram\",}\n",
    "\n",
    "# Stop words\n",
    "stop_words = stopwords.words('english')\n",
    "addstops = [\"among\", \"onto\", \"shall\", \"thrice\", \"thus\", \"twice\", \"unto\", \"us\", \"would\"]\n",
    "allstops = set(stop_words + addstops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thực hiện viết và gọi hàm cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "b45YnmSoUm1g"
   },
   "outputs": [],
   "source": [
    "tokenizer_tweet=TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "regexp= RegexpTokenizer(r'\\w+')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "truncated_words_keys=list(truncated_words_dict)\n",
    "def cleanData(text):\n",
    "  scripts_remove=[\"http\\S+\",        # Xóa địa chỉ web\n",
    "                  \"<.*?>\",           # Xóa các tag\n",
    "                  \"- Full.*\",       # Các nội dung văn bản có xuất hiện ký tự - Full cho đến hết văn bản là nội dung KHÔNG CẦN THIẾT\n",
    "                  \"[^\\x00-\\x7F]_?\", # Ký tự không thể decode về utf-8\n",
    "                  \"@\\w+\",           # Xóa các mention\n",
    "                  \"#\\w+\",           # Xóa các topic\n",
    "                  \"\\t\",             # Xóa tab\n",
    "                  \"\\n\"]             # Xóa xuống dòng\n",
    "\n",
    "  # Xóa ký tự RT ở đầu văn bản\n",
    "  text = re.sub(r'^RT\\s*', ' ', text)\n",
    "\n",
    "  # Xóa ký tự RT ở giữa văn bản\n",
    "  text = re.sub(r'\\bRT\\b', '', text)\n",
    "\n",
    "  # Thực hiện các thao tác trong scripts_remove\n",
    "  for script in scripts_remove:\n",
    "    text = re.sub(script, \"\", text)\n",
    "\n",
    "  #  Xóa các khoảng trắng bị thừa\n",
    "  text = re.sub(\"\\s+\", \" \", text)\n",
    "  \n",
    "  # Loại bỏ ký tự trắng bị thừa ở trước và sau\n",
    "  text = text.strip()\n",
    "\n",
    "  # tokenize tweets\n",
    "  tweet_tokens=tokenizer_tweet.tokenize(text)\n",
    "\n",
    "  # Thay thế các  các từ viết tắt\n",
    "  truncated_words=[]\n",
    "  for word in tweet_tokens:\n",
    "      if word in truncated_words_keys:\n",
    "        truncated_words = truncated_words + truncated_words_dict[word].split()\n",
    "      else:\n",
    "        truncated_words = truncated_words + word.split()\n",
    "  text_converted = \" \".join(truncated_words)\n",
    "\n",
    "  # Xóa stop words và các dấu nối câu\n",
    "  tweet_tokens=tokenizer_tweet.tokenize(text_converted)\n",
    "  text_clean_stop_words=[]\n",
    "  for word in tweet_tokens:\n",
    "    if (word not in allstops) and (word not in string.punctuation) and (word not in emoticons):\n",
    "        text_clean_stop_words.append(word)\n",
    "  return text_clean_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lightGBM_df[\"text_tokens\"]=train_lightGBM_df.text.apply(cleanData)\n",
    "test_lightGBM_df[\"text_tokens\"]=test_lightGBM_df.text.apply(cleanData)\n",
    "# tweet_df[\"tweet_tokens\"]=tweet_df.text.apply(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deeds, reason, may, allah, forgive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, evacuation, orders, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, ruby, smoke, pours, school]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, cranes, holding, bridge, collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[control, wild, fires, california, even, north...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[m1, 94, 01:04, utc, 5km, volcano, hawaii]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[police, investigating, e-bike, collided, car,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[latest, homes, razed, northern, california, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                            text_tokens  \n",
       "0                  [deeds, reason, may, allah, forgive]  \n",
       "1         [forest, fire, near, la, ronge, sask, canada]  \n",
       "2     [residents, asked, shelter, place, notified, o...  \n",
       "3     [13,000, people, receive, evacuation, orders, ...  \n",
       "4        [got, sent, photo, ruby, smoke, pours, school]  \n",
       "...                                                 ...  \n",
       "7608  [two, giant, cranes, holding, bridge, collapse...  \n",
       "7609  [control, wild, fires, california, even, north...  \n",
       "7610         [m1, 94, 01:04, utc, 5km, volcano, hawaii]  \n",
       "7611  [police, investigating, e-bike, collided, car,...  \n",
       "7612  [latest, homes, razed, northern, california, w...  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nối các token lại để tạo thành câu/đoạn văn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = \" \".join([char for char in text if char not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lightGBM_df[\"text_clean\"]=train_lightGBM_df.text_tokens.apply(remove_punctuation)\n",
    "test_lightGBM_df[\"text_clean\"]=test_lightGBM_df.text_tokens.apply(remove_punctuation)\n",
    "# tweet_df[\"Tweet\"]=tweet_df.tweet_tokens.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deeds, reason, may, allah, forgive]</td>\n",
       "      <td>deeds reason may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, evacuation, orders, ...</td>\n",
       "      <td>13,000 people receive evacuation orders califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, ruby, smoke, pours, school]</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[two, giant, cranes, holding, bridge, collapse...</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[control, wild, fires, california, even, north...</td>\n",
       "      <td>control wild fires california even northern pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[m1, 94, 01:04, utc, 5km, volcano, hawaii]</td>\n",
       "      <td>m1 94 01:04 utc 5km volcano hawaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[police, investigating, e-bike, collided, car,...</td>\n",
       "      <td>police investigating e-bike collided car littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[latest, homes, razed, northern, california, w...</td>\n",
       "      <td>latest homes razed northern california wildfir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                            text_tokens  \\\n",
       "0                  [deeds, reason, may, allah, forgive]   \n",
       "1         [forest, fire, near, la, ronge, sask, canada]   \n",
       "2     [residents, asked, shelter, place, notified, o...   \n",
       "3     [13,000, people, receive, evacuation, orders, ...   \n",
       "4        [got, sent, photo, ruby, smoke, pours, school]   \n",
       "...                                                 ...   \n",
       "7608  [two, giant, cranes, holding, bridge, collapse...   \n",
       "7609  [control, wild, fires, california, even, north...   \n",
       "7610         [m1, 94, 01:04, utc, 5km, volcano, hawaii]   \n",
       "7611  [police, investigating, e-bike, collided, car,...   \n",
       "7612  [latest, homes, razed, northern, california, w...   \n",
       "\n",
       "                                             text_clean  \n",
       "0                        deeds reason may allah forgive  \n",
       "1                 forest fire near la ronge sask canada  \n",
       "2     residents asked shelter place notified officer...  \n",
       "3     13,000 people receive evacuation orders califo...  \n",
       "4                got sent photo ruby smoke pours school  \n",
       "...                                                 ...  \n",
       "7608  two giant cranes holding bridge collapse nearb...  \n",
       "7609  control wild fires california even northern pa...  \n",
       "7610                 m1 94 01:04 utc 5km volcano hawaii  \n",
       "7611  police investigating e-bike collided car littl...  \n",
       "7612  latest homes razed northern california wildfir...  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sửa chính tả cho các từ bị lỗi:\n",
    "- Nhận xét: Chưa được tối ưu và chính xác. \n",
    "    - Số lượng nhiều.\n",
    "    - Trong 1 từ  bị thiếu hoặc dư nhiều chữ\n",
    "\n",
    "$ \\implies$  Làm cho việc xác định, chỉnh sửa tốn thời gian, sẽ có hiện tượng tìm nhầm từ khác với nội dung ban đầu của người viết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(language=\"en\")\n",
    "def correct_spellings(text):\n",
    "    words_list=tokenizer_tweet.tokenize(text)\n",
    "    corrected_text = []\n",
    "    for word in words_list:\n",
    "        if word in spell.unknown(words_list):\n",
    "            word_corrected=spell.correction(word)\n",
    "            corrected_text.append(word_corrected if word_corrected is not None else '')\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    text_corrected = \" \".join(corrected_text)\n",
    "    return text_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thực hiện kiểm tra chính tả các từ trong câu:\n",
    "\n",
    "*Cell dưới đã được chạy trong lần đầu tiên và lưu kết quả vào file `train_corrected_text` và `test_corrected_text` để tiết kiệm thời gian*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                        deeds reason may allah forgive\n",
      "1                  forest fire near la range ask canada\n",
      "2     residents asked shelter place notified officer...\n",
      "3           people receive evacuation orders california\n",
      "4                got sent photo ruby smoke pours school\n",
      "5     update california why 20 closed directions due...\n",
      "6     heavy rain causes flash flooding streets manit...\n",
      "7                          i'm top hill see fire woods \n",
      "8     there's emergency evacuation happening buildin...\n",
      "9                       i'm afraid tornado coming area \n",
      "10                      three people died heat wave far\n",
      "11    haha south tampa getting flooded hah wait seco...\n",
      "12                           18 19 days i've lost count\n",
      "13                               ago mandar arrived ago\n",
      "14                 damage school bus 80 multi car crash\n",
      "15                                           what's man\n",
      "16                                          love fruits\n",
      "17                                        summer lovely\n",
      "18                                             car fast\n",
      "19                                                     \n",
      "20                                          ridiculous \n",
      "21                                          london cool\n",
      "22                                          love skiing\n",
      "23                                        wonderful day\n",
      "24                                                 lool\n",
      "25                                  way  can't eat shit\n",
      "26                                         ny last week\n",
      "27                                      love girlfriend\n",
      "28                                                 cool\n",
      "29                                           like pasta\n",
      "30                                                  end\n",
      "31                             wholesale markets ablaze\n",
      "32                               always try bring heavy\n",
      "33            breaking news nigerian flag set ablaze ba\n",
      "34                                    crying set ablaze\n",
      "35                 plus side look sky last night ablaze\n",
      "36    they've built much hype around new acquisition...\n",
      "37                           ine office asia set ablaze\n",
      "38    barbarous jamaica two cars set ablaze santa cr...\n",
      "39                                          ablaze lord\n",
      "40                                                check\n",
      "41                     outside ablaze alive dead inside\n",
      "42    awesome time visiting cfo head office cop site...\n",
      "43                                   sooo pumped ablaze\n",
      "44           wanted set chicago ablaze preaching  hotel\n",
      "45         gained 3 followers last week know stats grow\n",
      "46    west burned thousands wildfires ablaze califor...\n",
      "47    building perfect blacklist life leave streets ...\n",
      "48                                                check\n",
      "49    first night retainers quite weird better get u...\n",
      "Name: text_clean, dtype: object\n",
      "0                           happened terrible car crash\n",
      "1             heard different cities stay safe everyone\n",
      "2     forest fire spot pond geese fleeing across str...\n",
      "3                                   apocalypse lighting\n",
      "4                        typhoon  kills 28 china taipan\n",
      "5                             we're shaking  earthquake\n",
      "6     they'd probably still show life arsenal yester...\n",
      "7                                                   hey\n",
      "8                                              nice hat\n",
      "9                                                  fuck\n",
      "10                                            like cold\n",
      "11                                                 nooo\n",
      "12                                                 tell\n",
      "13                                                     \n",
      "14                                              awesome\n",
      "15    birmingham wholesale market ablaze bac news fi...\n",
      "16                              wear shorts race ablaze\n",
      "17    take makings marriage crisis sets nigerian twi...\n",
      "18                                                check\n",
      "19    spa im splitting personalities techies follow ...\n",
      "20                 beware world ablaze sierra leone gap\n",
      "21                   burning man ablaze turban diva via\n",
      "22    diss song people take 1 thing run shaking head...\n",
      "23    rape victim dies sets ablaze 16  girl died bur...\n",
      "24                                       setting ablaze\n",
      "25    bins front field house wer set ablaze day flam...\n",
      "26                        allons ablaze 2015 pull radio\n",
      "27    burning rahm let's hope city hall builds giant...\n",
      "28                      hurt eyes ablaze insulted anger\n",
      "29    accident cleared path eb pa 18 cranberry slow ...\n",
      "30    got love burning self damn curling wand  swear...\n",
      "31                           hate banging shit accident\n",
      "32    car recorder zeroed  car camera vehicle traffi...\n",
      "33                       coincidence still secrets past\n",
      "34    accident a near lewis kingston roundabout rather \n",
      "35    pretend feel certain way feeling become genuin...\n",
      "36               legal medical referral service call  2\n",
      "37    there's construction guy working disney store ...\n",
      "38        feel like i'm going accident teeth going come\n",
      "39    northbound junctions je ja currently delays 10...\n",
      "40            say met accident week super jelly dave up\n",
      "41    accident hit run cold 500 block se vista ter g...\n",
      "42                               happened accident like\n",
      "43    please donate spread word training accident le...\n",
      "44    please like share new page indoor trampoline p...\n",
      "45         foi oh  las vegas procure pirate bay que tem\n",
      "46    schoolboy  original mix excision skim sexism f...\n",
      "47                                       320 ir iceman \n",
      "48     happened nepal last intel team still way est ...\n",
      "49                                       320 ir iceman \n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_lightGBM_df.text_clean.iloc[:50].apply(correct_spellings))\n",
    "print(test_lightGBM_df.text_clean.iloc[:50].apply(correct_spellings))\n",
    "\n",
    "# train_lightGBM_df[\"Tweet\"]=train_lightGBM_df.text_clean.apply(correct_spellings)\n",
    "# test_lightGBM_df[\"Tweet\"]=test_lightGBM_df.text_clean.apply(correct_spellings)\n",
    "\n",
    "# train_lightGBM_df.to_csv(\"train_corrected_text.csv\",index=False)\n",
    "# test_lightGBM_df.to_csv(\"test_corrected_text.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lightGBM_df=pd.read_csv(\"datasets/train_corrected_text.csv\")\n",
    "test_lightGBM_df=pd.read_csv(\"datasets/test_corrected_text.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beside\", \"between\", \"by\", \"down\", \"during\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"near\", \"of\", \"off\", \"on\", \"out\", \"over\", \"through\", \"to\", \"toward\", \"under\", \"up\", \"with\"]\n",
    "prepositions_less_common = [\"aboard\", \"along\", \"amid\", \"as\", \"beneath\", \"beyond\", \"but\", \"concerning\", \"considering\", \"despite\", \"except\", \"following\", \"like\", \"minus\", \"onto\", \"outside\", \"per\", \"plus\", \"regarding\", \"round\", \"since\", \"than\", \"till\", \"underneath\", \"unlike\", \"until\", \"upon\", \"versus\", \"via\", \"within\", \"without\"]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\"both\", \"and\", \"either\", \"or\", \"neither\", \"nor\", \"not\", \"only\", \"but\", \"whether\", \"or\"]\n",
    "subordinating_conjunctions = [\"after\", \"although\", \"as\", \"because\", \"before\", \"if\", \"lest\", \"once\", \"only\", \"since\", \"so\", \"supposing\", \"that\", \"than\", \"though\", \"till\", \"unless\", \"until\", \"when\", \"whenever\", \"where\", \"whereas\", \"wherever\", \"while\"]\n",
    "additional_stops = alphabets + prepositions + prepositions_less_common + coordinating_conjunctions + correlative_conjunctions + subordinating_conjunctions\n",
    "\n",
    "def nomalize_text(text):\n",
    "  if not (text is np.nan):\n",
    "    # Rút gọn từ về dạng gốc\n",
    "    text_spacy = \" \".join([lemmatizer.lemmatize(word) for word in regexp.tokenize(text)])\n",
    "\n",
    "    # Xóa các từ có chữ số\n",
    "    word_list_non_alpha = [word for word in regexp.tokenize(text_spacy) if word.isalpha()]\n",
    "    text_non_alpha = \" \".join(word_list_non_alpha)\n",
    "    \n",
    "    # Xóa các stopwords\n",
    "    text_remove_addi_stopwords= \" \".join([word for word in regexp.tokenize(text_non_alpha) if word not in additional_stops])\n",
    "    return text_remove_addi_stopwords\n",
    "  return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "train_lightGBM_df[\"Tweet_nomalize_text\"]=train_lightGBM_df.Tweet.apply(nomalize_text)\n",
    "test_lightGBM_df[\"Tweet_nomalize_text\"]=test_lightGBM_df.Tweet.apply(nomalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lightGBM_df=train_lightGBM_df[['id', 'keyword', 'location', 'text', 'text_tokens','text_clean', 'Tweet', 'Tweet_nomalize_text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>['deeds', 'reason', 'may', 'allah', 'forgive']</td>\n",
       "      <td>deeds reason may allah forgive</td>\n",
       "      <td>deeds reason may allah forgive</td>\n",
       "      <td>deed reason may allah forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>['forest', 'fire', 'near', 'la', 'ronge', 'sas...</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la range ask canada</td>\n",
       "      <td>forest fire la range ask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>['residents', 'asked', 'shelter', 'place', 'no...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>['13,000', 'people', 'receive', 'evacuation', ...</td>\n",
       "      <td>13,000 people receive evacuation orders califo...</td>\n",
       "      <td>people receive evacuation orders california</td>\n",
       "      <td>people receive evacuation order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>['got', 'sent', 'photo', 'ruby', 'smoke', 'pou...</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0     ['deeds', 'reason', 'may', 'allah', 'forgive']   \n",
       "1  ['forest', 'fire', 'near', 'la', 'ronge', 'sas...   \n",
       "2  ['residents', 'asked', 'shelter', 'place', 'no...   \n",
       "3  ['13,000', 'people', 'receive', 'evacuation', ...   \n",
       "4  ['got', 'sent', 'photo', 'ruby', 'smoke', 'pou...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                     deeds reason may allah forgive   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  13,000 people receive evacuation orders califo...   \n",
       "4             got sent photo ruby smoke pours school   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0                     deeds reason may allah forgive   \n",
       "1               forest fire near la range ask canada   \n",
       "2  residents asked shelter place notified officer...   \n",
       "3        people receive evacuation orders california   \n",
       "4             got sent photo ruby smoke pours school   \n",
       "\n",
       "                                 Tweet_nomalize_text  target  \n",
       "0                      deed reason may allah forgive       1  \n",
       "1                    forest fire la range ask canada       1  \n",
       "2  resident asked shelter place notified officer ...       1  \n",
       "3         people receive evacuation order california       1  \n",
       "4             got sent photo ruby smoke pours school       1  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>['happened', 'terrible', 'car', 'crash']</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>['heard', 'different', 'cities', 'stay', 'safe...</td>\n",
       "      <td>heard different cities stay safe everyone</td>\n",
       "      <td>heard different cities stay safe everyone</td>\n",
       "      <td>heard different city stay safe everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>['forest', 'fire', 'spot', 'pond', 'geese', 'f...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>forest fire spot pond goose fleeing street can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>['apocalypse', 'lighting']</td>\n",
       "      <td>apocalypse lighting</td>\n",
       "      <td>apocalypse lighting</td>\n",
       "      <td>apocalypse lighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>['typhoon', 'soudelor', 'kills', '28', 'china'...</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "      <td>typhoon  kills 28 china taipan</td>\n",
       "      <td>typhoon kill china taipan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0           ['happened', 'terrible', 'car', 'crash']   \n",
       "1  ['heard', 'different', 'cities', 'stay', 'safe...   \n",
       "2  ['forest', 'fire', 'spot', 'pond', 'geese', 'f...   \n",
       "3                         ['apocalypse', 'lighting']   \n",
       "4  ['typhoon', 'soudelor', 'kills', '28', 'china'...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                        happened terrible car crash   \n",
       "1          heard different cities stay safe everyone   \n",
       "2  forest fire spot pond geese fleeing across str...   \n",
       "3                                apocalypse lighting   \n",
       "4             typhoon soudelor kills 28 china taiwan   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0                        happened terrible car crash   \n",
       "1          heard different cities stay safe everyone   \n",
       "2  forest fire spot pond geese fleeing across str...   \n",
       "3                                apocalypse lighting   \n",
       "4                     typhoon  kills 28 china taipan   \n",
       "\n",
       "                                 Tweet_nomalize_text  \n",
       "0                        happened terrible car crash  \n",
       "1            heard different city stay safe everyone  \n",
       "2  forest fire spot pond goose fleeing street can...  \n",
       "3                                apocalypse lighting  \n",
       "4                          typhoon kill china taipan  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lightGBM_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xem xét DataFrame ngoài lề"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>6354</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>6372</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>6375</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>6394</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>NAWF SIDE POKING OUT</td>\n",
       "      <td>@Hurricane_Dame ???????? I don't have them the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>7167</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>London</td>\n",
       "      <td>@SophieWisey I couldn't. #mudslide</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6766</th>\n",
       "      <td>9697</td>\n",
       "      <td>tornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Ayshun_Tornado then don't</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword               location  \\\n",
       "4468  6354   hostages                    NaN   \n",
       "4480  6372   hostages                    NaN   \n",
       "4483  6375   hostages                    NaN   \n",
       "4497  6394  hurricane  NAWF SIDE POKING OUT    \n",
       "5026  7167   mudslide                 London   \n",
       "6766  9697    tornado                    NaN   \n",
       "\n",
       "                                                   text text_tokens  \\\n",
       "4468  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...          []   \n",
       "4480  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...          []   \n",
       "4483  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...          []   \n",
       "4497  @Hurricane_Dame ???????? I don't have them the...          []   \n",
       "5026                 @SophieWisey I couldn't. #mudslide          []   \n",
       "6766                         @Ayshun_Tornado then don't          []   \n",
       "\n",
       "     text_clean Tweet Tweet_nomalize_text  target  \n",
       "4468        NaN   NaN                 NaN       1  \n",
       "4480        NaN   NaN                 NaN       1  \n",
       "4483        NaN   NaN                 NaN       1  \n",
       "4497        NaN   NaN                 NaN       1  \n",
       "5026        NaN   NaN                 NaN       1  \n",
       "6766        NaN   NaN                 NaN       0  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df[train_lightGBM_df.text_clean.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>['goooaaal']</td>\n",
       "      <td>goooaaal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>163</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Aftershock https://t.co/xMWODFMtUI</td>\n",
       "      <td>['aftershock']</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>190</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock https://t.co/jV8ppKhJY7</td>\n",
       "      <td>['aftershock']</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>6354</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>6372</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>6375</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>6394</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>NAWF SIDE POKING OUT</td>\n",
       "      <td>@Hurricane_Dame ???????? I don't have them the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>7167</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>London</td>\n",
       "      <td>@SophieWisey I couldn't. #mudslide</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6766</th>\n",
       "      <td>9697</td>\n",
       "      <td>tornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Ayshun_Tornado then don't</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword               location  \\\n",
       "19      28         NaN                    NaN   \n",
       "113    163  aftershock                Belgium   \n",
       "131    190  aftershock                    NaN   \n",
       "4468  6354    hostages                    NaN   \n",
       "4480  6372    hostages                    NaN   \n",
       "4483  6375    hostages                    NaN   \n",
       "4497  6394   hurricane  NAWF SIDE POKING OUT    \n",
       "5026  7167    mudslide                 London   \n",
       "6766  9697     tornado                    NaN   \n",
       "\n",
       "                                                   text     text_tokens  \\\n",
       "19                         What a goooooooaaaaaal!!!!!!    ['goooaaal']   \n",
       "113                  Aftershock https://t.co/xMWODFMtUI  ['aftershock']   \n",
       "131                  Aftershock https://t.co/jV8ppKhJY7  ['aftershock']   \n",
       "4468  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...              []   \n",
       "4480  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...              []   \n",
       "4483  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...              []   \n",
       "4497  @Hurricane_Dame ???????? I don't have them the...              []   \n",
       "5026                 @SophieWisey I couldn't. #mudslide              []   \n",
       "6766                         @Ayshun_Tornado then don't              []   \n",
       "\n",
       "      text_clean Tweet Tweet_nomalize_text  target  \n",
       "19      goooaaal   NaN                 NaN       0  \n",
       "113   aftershock   NaN                 NaN       0  \n",
       "131   aftershock   NaN                 NaN       0  \n",
       "4468         NaN   NaN                 NaN       1  \n",
       "4480         NaN   NaN                 NaN       1  \n",
       "4483         NaN   NaN                 NaN       1  \n",
       "4497         NaN   NaN                 NaN       1  \n",
       "5026         NaN   NaN                 NaN       1  \n",
       "6766         NaN   NaN                 NaN       0  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df[train_lightGBM_df.Tweet.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>['goooaaal']</td>\n",
       "      <td>goooaaal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>163</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>Aftershock https://t.co/xMWODFMtUI</td>\n",
       "      <td>['aftershock']</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>190</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock https://t.co/jV8ppKhJY7</td>\n",
       "      <td>['aftershock']</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>6354</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>6372</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>6375</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>6394</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>NAWF SIDE POKING OUT</td>\n",
       "      <td>@Hurricane_Dame ???????? I don't have them the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>7167</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>London</td>\n",
       "      <td>@SophieWisey I couldn't. #mudslide</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6766</th>\n",
       "      <td>9697</td>\n",
       "      <td>tornado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Ayshun_Tornado then don't</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword               location  \\\n",
       "19      28         NaN                    NaN   \n",
       "113    163  aftershock                Belgium   \n",
       "131    190  aftershock                    NaN   \n",
       "4468  6354    hostages                    NaN   \n",
       "4480  6372    hostages                    NaN   \n",
       "4483  6375    hostages                    NaN   \n",
       "4497  6394   hurricane  NAWF SIDE POKING OUT    \n",
       "5026  7167    mudslide                 London   \n",
       "6766  9697     tornado                    NaN   \n",
       "\n",
       "                                                   text     text_tokens  \\\n",
       "19                         What a goooooooaaaaaal!!!!!!    ['goooaaal']   \n",
       "113                  Aftershock https://t.co/xMWODFMtUI  ['aftershock']   \n",
       "131                  Aftershock https://t.co/jV8ppKhJY7  ['aftershock']   \n",
       "4468  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...              []   \n",
       "4480  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...              []   \n",
       "4483  No #news of #hostages in #Libya\\r\\n\\r\\nhttp://...              []   \n",
       "4497  @Hurricane_Dame ???????? I don't have them the...              []   \n",
       "5026                 @SophieWisey I couldn't. #mudslide              []   \n",
       "6766                         @Ayshun_Tornado then don't              []   \n",
       "\n",
       "      text_clean Tweet Tweet_nomalize_text  target  \n",
       "19      goooaaal   NaN                 NaN       0  \n",
       "113   aftershock   NaN                 NaN       0  \n",
       "131   aftershock   NaN                 NaN       0  \n",
       "4468         NaN   NaN                 NaN       1  \n",
       "4480         NaN   NaN                 NaN       1  \n",
       "4483         NaN   NaN                 NaN       1  \n",
       "4497         NaN   NaN                 NaN       1  \n",
       "5026         NaN   NaN                 NaN       1  \n",
       "6766         NaN   NaN                 NaN       0  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df[train_lightGBM_df.Tweet_nomalize_text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What a goooooooaaaaaal!!!!!!',\n",
       " 'Aftershock https://t.co/xMWODFMtUI',\n",
       " 'Aftershock https://t.co/jV8ppKhJY7',\n",
       " 'No #news of #hostages in #Libya\\r\\n\\r\\nhttp://t.co/eXil1bKzmP\\r\\n\\r\\n#India #terrorism #Africa #AP #TS #NRI #News #TRS #TDP #BJP http://t.co/ehomn68oJB',\n",
       " 'No #news of #hostages in #Libya\\r\\n\\r\\nhttp://t.co/bjjOIfzUhL\\r\\n\\r\\n#India #terrorism #Africa #AP #TS #NRI #News #TRS #TDP #BJP http://t.co/IywZAlLsN4',\n",
       " 'No #news of #hostages in #Libya\\r\\n\\r\\nhttp://t.co/k9FBtcCU58\\r\\n\\r\\n#India #terrorism #Africa #AP #TS #NRI #News #TRS #TDP #BJP http://t.co/XYj0rPsAI2',\n",
       " \"@Hurricane_Dame ???????? I don't have them they out here\",\n",
       " \"@SophieWisey I couldn't. #mudslide\",\n",
       " \"@Ayshun_Tornado then don't\"]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df[train_lightGBM_df.Tweet.isna()].text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>6335</td>\n",
       "      <td>hostages</td>\n",
       "      <td>THANJAVUR</td>\n",
       "      <td>2 hostages in Libya remain unharmed: Governmen...</td>\n",
       "      <td>['2', 'hostages', 'libya', 'remain', 'unharmed...</td>\n",
       "      <td>2 hostages libya remain unharmed government so...</td>\n",
       "      <td>2 hostages libya remain unharmed government so...</td>\n",
       "      <td>hostage libya remain unharmed government sourc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>6336</td>\n",
       "      <td>hostages</td>\n",
       "      <td>Japan</td>\n",
       "      <td>#hot  C-130 specially modified to land in a st...</td>\n",
       "      <td>['c', '130', 'specially', 'modified', 'land', ...</td>\n",
       "      <td>c 130 specially modified land stadium rescue h...</td>\n",
       "      <td>i 130 specially modified land stadium rescue h...</td>\n",
       "      <td>specially modified land stadium rescue hostage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>6337</td>\n",
       "      <td>hostages</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>#hot  C-130 specially modified to land in a st...</td>\n",
       "      <td>['c', '130', 'specially', 'modified', 'land', ...</td>\n",
       "      <td>c 130 specially modified land stadium rescue h...</td>\n",
       "      <td>i 130 specially modified land stadium rescue h...</td>\n",
       "      <td>specially modified land stadium rescue hostage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>6338</td>\n",
       "      <td>hostages</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@hannahkauthor Read: American lives first | Th...</td>\n",
       "      <td>['read', 'american', 'lives', 'first', 'chroni...</td>\n",
       "      <td>read american lives first chronicle held</td>\n",
       "      <td>read american lives first chronicle held</td>\n",
       "      <td>read american life first chronicle held</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>6339</td>\n",
       "      <td>hostages</td>\n",
       "      <td>Cumming, GA</td>\n",
       "      <td>C-130 specially modified to land in a stadium ...</td>\n",
       "      <td>['c', '130', 'specially', 'modified', 'land', ...</td>\n",
       "      <td>c 130 specially modified land stadium rescue h...</td>\n",
       "      <td>i 130 specially modified land stadium rescue h...</td>\n",
       "      <td>specially modified land stadium rescue hostage...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   keyword       location  \\\n",
       "4452  6335  hostages      THANJAVUR   \n",
       "4453  6336  hostages          Japan   \n",
       "4454  6337  hostages  Las Vegas, NV   \n",
       "4455  6338  hostages            NaN   \n",
       "4456  6339  hostages    Cumming, GA   \n",
       "\n",
       "                                                   text  \\\n",
       "4452  2 hostages in Libya remain unharmed: Governmen...   \n",
       "4453  #hot  C-130 specially modified to land in a st...   \n",
       "4454  #hot  C-130 specially modified to land in a st...   \n",
       "4455  @hannahkauthor Read: American lives first | Th...   \n",
       "4456  C-130 specially modified to land in a stadium ...   \n",
       "\n",
       "                                            text_tokens  \\\n",
       "4452  ['2', 'hostages', 'libya', 'remain', 'unharmed...   \n",
       "4453  ['c', '130', 'specially', 'modified', 'land', ...   \n",
       "4454  ['c', '130', 'specially', 'modified', 'land', ...   \n",
       "4455  ['read', 'american', 'lives', 'first', 'chroni...   \n",
       "4456  ['c', '130', 'specially', 'modified', 'land', ...   \n",
       "\n",
       "                                             text_clean  \\\n",
       "4452  2 hostages libya remain unharmed government so...   \n",
       "4453  c 130 specially modified land stadium rescue h...   \n",
       "4454  c 130 specially modified land stadium rescue h...   \n",
       "4455           read american lives first chronicle held   \n",
       "4456  c 130 specially modified land stadium rescue h...   \n",
       "\n",
       "                                                  Tweet  \\\n",
       "4452  2 hostages libya remain unharmed government so...   \n",
       "4453  i 130 specially modified land stadium rescue h...   \n",
       "4454  i 130 specially modified land stadium rescue h...   \n",
       "4455           read american lives first chronicle held   \n",
       "4456  i 130 specially modified land stadium rescue h...   \n",
       "\n",
       "                                    Tweet_nomalize_text  target  \n",
       "4452  hostage libya remain unharmed government sourc...       1  \n",
       "4453  specially modified land stadium rescue hostage...       1  \n",
       "4454  specially modified land stadium rescue hostage...       1  \n",
       "4455            read american life first chronicle held       1  \n",
       "4456  specially modified land stadium rescue hostage...       1  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df[train_lightGBM_df.keyword==\"hostages\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_nomalize_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>146</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Instagram - @heyimginog</td>\n",
       "      <td>@afterShock_DeLo scuf ps live and the game... cya</td>\n",
       "      <td>['scuf', 'ps', 'live', 'game', '...', 'cya']</td>\n",
       "      <td>scuf ps live game ... cya</td>\n",
       "      <td>scum is live game  ya</td>\n",
       "      <td>scum is live game ya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>149</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>304</td>\n",
       "      <td>'The man who can drive himself further once th...</td>\n",
       "      <td>['man', 'drive', 'effort', 'gets', 'painful', ...</td>\n",
       "      <td>man drive effort gets painful man win roger ba...</td>\n",
       "      <td>man drive effort gets painful man win roger ba...</td>\n",
       "      <td>man drive effort get painful man win roger ban...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>151</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yN...</td>\n",
       "      <td>['320', 'ir', 'icemoon', 'aftershock']</td>\n",
       "      <td>320 ir icemoon aftershock</td>\n",
       "      <td>320 ir iceman</td>\n",
       "      <td>ir iceman</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>153</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>304</td>\n",
       "      <td>'There is no victory at bargain basement price...</td>\n",
       "      <td>['victory', 'bargain', 'basement', 'prices', '...</td>\n",
       "      <td>victory bargain basement prices dwight david e...</td>\n",
       "      <td>victory bargain basement prices dwight david e...</td>\n",
       "      <td>victory bargain basement price dwight david ei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>156</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>US</td>\n",
       "      <td>320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...</td>\n",
       "      <td>['320', 'ir', 'icemoon', 'aftershock']</td>\n",
       "      <td>320 ir icemoon aftershock</td>\n",
       "      <td>320 ir iceman</td>\n",
       "      <td>ir iceman</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     keyword                  location  \\\n",
       "102  146  aftershock  Instagram - @heyimginog    \n",
       "103  149  aftershock                       304   \n",
       "104  151  aftershock               Switzerland   \n",
       "105  153  aftershock                       304   \n",
       "106  156  aftershock                        US   \n",
       "\n",
       "                                                  text  \\\n",
       "102  @afterShock_DeLo scuf ps live and the game... cya   \n",
       "103  'The man who can drive himself further once th...   \n",
       "104  320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/yN...   \n",
       "105  'There is no victory at bargain basement price...   \n",
       "106  320 [IR] ICEMOON [AFTERSHOCK] | http://t.co/vA...   \n",
       "\n",
       "                                           text_tokens  \\\n",
       "102       ['scuf', 'ps', 'live', 'game', '...', 'cya']   \n",
       "103  ['man', 'drive', 'effort', 'gets', 'painful', ...   \n",
       "104             ['320', 'ir', 'icemoon', 'aftershock']   \n",
       "105  ['victory', 'bargain', 'basement', 'prices', '...   \n",
       "106             ['320', 'ir', 'icemoon', 'aftershock']   \n",
       "\n",
       "                                            text_clean  \\\n",
       "102                          scuf ps live game ... cya   \n",
       "103  man drive effort gets painful man win roger ba...   \n",
       "104                          320 ir icemoon aftershock   \n",
       "105  victory bargain basement prices dwight david e...   \n",
       "106                          320 ir icemoon aftershock   \n",
       "\n",
       "                                                 Tweet  \\\n",
       "102                              scum is live game  ya   \n",
       "103  man drive effort gets painful man win roger ba...   \n",
       "104                                     320 ir iceman    \n",
       "105  victory bargain basement prices dwight david e...   \n",
       "106                                     320 ir iceman    \n",
       "\n",
       "                                   Tweet_nomalize_text  target  \n",
       "102                               scum is live game ya       0  \n",
       "103  man drive effort get painful man win roger ban...       0  \n",
       "104                                          ir iceman       0  \n",
       "105  victory bargain basement price dwight david ei...       0  \n",
       "106                                          ir iceman       0  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lightGBM_df[train_lightGBM_df.keyword==\"aftershock\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chia dữ liệu thành 2 tập train - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(train_lightGBM_df[\"Tweet_nomalize_text\"]),np.array(train_lightGBM_df[\"target\"]), test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['new weapon cause unimaginable destruction',\n",
       "       'thing got soaked deluge going pad tampon thanks',\n",
       "       'it col police catch pickpocket liverpool street', ...,\n",
       "       'mass murderer che greeting woman north korea',\n",
       "       'woman flower printed shoulder handbag cross body metal chain satchel bag blue',\n",
       "       'nuclear bomb terrible weapon'], dtype=object)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Transform dữ liệu bằng phương pháp tf-idf trước khi đưa vào mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF Model: Train features shape:(5709, 8281) and Test features shape:(1904, 8281)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tfidf = TfidfVectorizer(use_idf=True, tokenizer=word_tokenize,min_df=0.00002,max_df=0.70)\n",
    "X_train_tf = tfidf.fit_transform(X_train.astype('U'))\n",
    "X_test_tf = tfidf.transform(X_test.astype('U'))\n",
    "\n",
    "print(f\"TF_IDF Model: Train features shape:{X_train_tf.shape} and Test features shape:{X_test_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy thử với các mô hình: SVM và LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM  - accuracy:  0.7752100840336135\n",
      "Support Vector Machine  - accuracy:  0.7993697478991597\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(random_state=42)\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "clfs = {\n",
    "    \"LightGBM\": lgb,\n",
    "    \"Support Vector Machine\":svc,\n",
    "}\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    clf.fit(X_train_tf, y_train)\n",
    "    y_pred = clf.predict(X_test_tf)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print(name, \" - accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giữa 2 model cổ điển, ta chọn SVM để dự đoán dữ liệu đề cho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7379)\t0.6137496057222812\n",
      "  (0, 3296)\t0.542184614520603\n",
      "  (0, 1636)\t0.4067071891259731\n",
      "  (0, 1094)\t0.4048907600404576\n",
      "  (1, 7007)\t0.40243950313571647\n",
      "  (1, 6322)\t0.49977611803124505\n",
      "  (1, 3356)\t0.3799678448491391\n",
      "  (1, 2515)\t0.35614504965626204\n",
      "  (1, 2008)\t0.44436010141343807\n",
      "  (1, 1299)\t0.3458258505655336\n",
      "  (2, 7072)\t0.3264296995154451\n",
      "  (2, 6937)\t0.32048897742192195\n",
      "  (2, 6396)\t0.3186546719465086\n",
      "  (2, 5536)\t0.4168021797780186\n",
      "  (2, 5000)\t0.36577921325023366\n",
      "  (2, 2876)\t0.2884110767579541\n",
      "  (2, 2805)\t0.43131224257441936\n",
      "  (2, 2767)\t0.2092522850236929\n",
      "  (2, 1062)\t0.262056211869531\n",
      "  (3, 4242)\t0.7909928275192194\n",
      "  (3, 299)\t0.6118254218428247\n",
      "  (4, 7700)\t0.48722003570687034\n",
      "  (4, 7285)\t0.5600537690020883\n",
      "  (4, 4036)\t0.44209102182194965\n",
      "  (4, 1257)\t0.503499693204252\n",
      "  :\t:\n",
      "  (3258, 251)\t0.43681821614205874\n",
      "  (3259, 8180)\t0.3669912353758528\n",
      "  (3259, 7053)\t0.26312620270955156\n",
      "  (3259, 7034)\t0.2556053108974236\n",
      "  (3259, 5586)\t0.32122386712319373\n",
      "  (3259, 5184)\t0.34673885229386475\n",
      "  (3259, 4336)\t0.2556053108974236\n",
      "  (3259, 4127)\t0.2758576939794117\n",
      "  (3259, 3572)\t0.3192648504491446\n",
      "  (3259, 3447)\t0.2896927745635892\n",
      "  (3259, 1299)\t0.28561307759237126\n",
      "  (3259, 796)\t0.31381050715564746\n",
      "  (3260, 4260)\t0.47944319020150716\n",
      "  (3260, 3184)\t0.515979227051662\n",
      "  (3260, 1919)\t0.4598883567029226\n",
      "  (3260, 1244)\t0.5407424192623104\n",
      "  (3261, 8034)\t0.40947958871666984\n",
      "  (3261, 5201)\t0.5229212594099368\n",
      "  (3261, 3854)\t0.4335030542637382\n",
      "  (3261, 3533)\t0.43067094447243764\n",
      "  (3261, 3338)\t0.43067094447243764\n",
      "  (3262, 5476)\t0.42516851851209014\n",
      "  (3262, 4849)\t0.5667134142055723\n",
      "  (3262, 2374)\t0.3684939859737198\n",
      "  (3262, 56)\t0.6018968510690995\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'happened terrible car crash'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[256], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m X_test_tf_ \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(X_test_\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_test_tf_)\n\u001b[1;32m----> 6\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\svm\\_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\svm\\_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    610\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 613\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[0;32m    623\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'happened terrible car crash'"
     ]
    }
   ],
   "source": [
    "X_test_ = np.array(test_lightGBM_df[\"Tweet_nomalize_text\"])\n",
    "\n",
    "X_test_tf_ = tfidf.transform(X_test_.astype('U'))\n",
    "print(X_test_tf_)\n",
    "\n",
    "y_hat = clf.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       hunger hit can function probably last long fam...\n",
       "1       lois girl advertised new friend replace best f...\n",
       "2       avoid wearing dead black flaming red stark whi...\n",
       "3       tell cousin wanna hang text saying we re comin...\n",
       "4       far right racist call destruction previously a...\n",
       "                              ...                        \n",
       "3041    louis button monogram shoulder bag cross body bag\n",
       "3042    specially modified land stadium rescue hostage...\n",
       "3043                                all pussy shook death\n",
       "3044                             took way longer expected\n",
       "3045                   haha alright stupid he traumatised\n",
       "Name: tweet, Length: 3043, dtype: object"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = test_lightGBM_df['id']\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array length 1904 does not match index length 3263",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[245], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m Submission \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m Submission\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:680\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    676\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    677\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    678\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m         )\n\u001b[1;32m--> 680\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    682\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: array length 1904 does not match index length 3263"
     ]
    }
   ],
   "source": [
    "id = id.astype('int64')\n",
    "Submission = pd.DataFrame({'id':id,\n",
    "                           'target':y_pred})\n",
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lightGBM_df.dropna(subset=['Tweet_nomalize_text'], inplace = True)\n",
    "# function to flatten one list\n",
    "def flat_list(unflat_list):\n",
    "    flatted = [item for sublist in unflat_list for item in sublist]\n",
    "    return flatted\n",
    "\n",
    "def to_list(df, attribute):\n",
    "    # Select the normalised transcript column \n",
    "    df_transcription = df[[attribute]]\n",
    "    # To convert the attribute into list format, but it has inner list. So it cannot put into the CountVectoriser\n",
    "    unflat_list_transcription = df_transcription.values.tolist()\n",
    "    # Let's use back the function defined above, \"flat_list\", to flatten the list\n",
    "    flat_list_transcription = flat_list(unflat_list_transcription)\n",
    "    return flat_list_transcription\n",
    "flat_list_transcription = to_list(train_lightGBM_df, 'Tweet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unigram', 'unigram_bigram', 'bigram', 'bigram_trigram', 'trigram']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_features ={'unigram':(1,1),'unigram_bigram':(1,2),'bigram':(2,2),\\\n",
    "       'bigram_trigram':(2,3),'trigram':(3,3)}\n",
    "feature_name=[]\n",
    "temp=[]\n",
    "for key, values in n_gram_features.items():\n",
    "    temp.append(key)\n",
    "    feature_name.append(key)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_n_gram_features(flat_list_transcription):\n",
    "    temp=[]\n",
    "    for key, values in n_gram_features.items(): \n",
    "        vectorizer = CountVectorizer(ngram_range=values)\n",
    "        vectorizer.fit(flat_list_transcription)\n",
    "        temp.append(vectorizer.transform(flat_list_transcription))\n",
    "    return temp\n",
    "temp = generate_n_gram_features(flat_list_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N-Gram Feature Vector</th>\n",
       "      <th>Data Dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram</td>\n",
       "      <td>(7604, 11131)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unigram_bigram</td>\n",
       "      <td>(7604, 51199)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigram</td>\n",
       "      <td>(7604, 40068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bigram_trigram</td>\n",
       "      <td>(7604, 77572)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trigram</td>\n",
       "      <td>(7604, 37504)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  N-Gram Feature Vector Data Dimension\n",
       "0               unigram  (7604, 11131)\n",
       "1        unigram_bigram  (7604, 51199)\n",
       "2                bigram  (7604, 40068)\n",
       "3        bigram_trigram  (7604, 77572)\n",
       "4               trigram  (7604, 37504)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = {'unigram':temp[0], \n",
    "              'unigram_bigram':temp[1], \n",
    "              'bigram':temp[2], \n",
    "              'bigram_trigram':temp[3], \n",
    "              'trigram':temp[4]}\n",
    "feature_vector = [] ; feature_vector_shape = []\n",
    "for key in dataframes:\n",
    "    feature_vector.append(key)\n",
    "    feature_vector_shape.append(dataframes[key].shape)\n",
    "\n",
    "n_gram_df = pd.DataFrame({'N-Gram Feature Vector':feature_vector, 'Data Dimension':feature_vector_shape})\n",
    "n_gram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7604x11131 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 59919 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes['unigram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state_number =8888\n",
    "df_target =train_lightGBM_df['target'].values\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_target1 = train_lightGBM_df[['target']].values.ravel()\n",
    "# df_target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "metrics = {\n",
    "    'f1':[f1_score, 'f1_macro'], \n",
    "    'precision': [precision_score, 'precision_macro'], \n",
    "    'recall': [recall_score, 'recall_macro']\n",
    "}\n",
    "\n",
    "# get evaluation result\n",
    "\n",
    "def get_performance(param_grid, base_estimator, dataframes):\n",
    "    df_name_list =[]; best_estimator_list=[]; best_score_list=[]; test_predict_result_list=[];\n",
    "    metric_list = [];\n",
    "    \n",
    "    for df_name, df in dataframes.items():\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_lightGBM_df, df_target, test_size=0.2, random_state=random_state_number)\n",
    "        for _, metric_dict in metrics.items():\n",
    "            sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5, scoring=metric_dict[1],random_state=random_state_number,\n",
    "                                      factor=2).fit(X_train, y_train)\n",
    "\n",
    "            best_estimator = sh.best_estimator_\n",
    "            clf = best_estimator.fit(X_train, y_train)\n",
    "            prediction = clf.predict(X_test)\n",
    "            test_predict_result = metric_dict[0](y_test, prediction, average='macro')\n",
    "\n",
    "            df_name_list.append(df_name) ; best_estimator_list.append(best_estimator) ; \n",
    "            best_score_list.append(sh.best_score_) ; \n",
    "            test_predict_result_list.append(test_predict_result) ;metric_list.append(metric_dict[1])\n",
    "            \n",
    "            \n",
    "    model_result = pd.DataFrame({'Vector':df_name_list,'Metric':metric_list,\n",
    "                               'Calibrated Estimator':best_estimator_list,\n",
    "                               'Best CV Metric Score':best_score_list, 'Test Predict Metric Score': test_predict_result_list})\n",
    "    return model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 240 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n144 fits failed with the following error:\nTraceback (most recent call last):\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\base.py\", line 565, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'explosion'\n\n--------------------------------------------------------------------------------\n96 fits failed with the following error:\nTraceback (most recent call last):\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\base.py\", line 565, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'obliteration'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[240], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m35\u001b[39m,\u001b[38;5;241m37\u001b[39m,\u001b[38;5;241m38\u001b[39m,\u001b[38;5;241m39\u001b[39m,\u001b[38;5;241m40\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m170\u001b[39m,\u001b[38;5;241m180\u001b[39m,\u001b[38;5;241m190\u001b[39m,\u001b[38;5;241m200\u001b[39m]}\n\u001b[0;32m      2\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39mrandom_state_number)\n\u001b[1;32m----> 3\u001b[0m rfc_result \u001b[38;5;241m=\u001b[39m \u001b[43mget_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m rfc_result\n",
      "Cell \u001b[1;32mIn[239], line 29\u001b[0m, in \u001b[0;36mget_performance\u001b[1;34m(param_grid, base_estimator, dataframes)\u001b[0m\n\u001b[0;32m     26\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(train_lightGBM_df, df_target, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mrandom_state_number)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, metric_dict \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     28\u001b[0m     sh \u001b[38;5;241m=\u001b[39m \u001b[43mHalvingGridSearchCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m---> 29\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     best_estimator \u001b[38;5;241m=\u001b[39m sh\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     32\u001b[0m     clf \u001b[38;5;241m=\u001b[39m best_estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:273\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_parameters(\n\u001b[0;32m    266\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    267\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    268\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    269\u001b[0m )\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples_orig \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_results_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_test_score\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_index_]\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_search_successive_halving.py:378\u001b[0m, in \u001b[0;36mBaseSuccessiveHalving._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m    371\u001b[0m     cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checked_cv_orig\n\u001b[0;32m    373\u001b[0m more_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: [itr] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: [n_resources] \u001b[38;5;241m*\u001b[39m n_candidates,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmore_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmore_results\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m n_candidates_to_keep \u001b[38;5;241m=\u001b[39m ceil(n_candidates \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactor)\n\u001b[0;32m    383\u001b[0m candidate_params \u001b[38;5;241m=\u001b[39m _top_k(results, n_candidates_to_keep, itr)\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mD:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 240 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n144 fits failed with the following error:\nTraceback (most recent call last):\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\base.py\", line 565, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'explosion'\n\n--------------------------------------------------------------------------------\n96 fits failed with the following error:\nTraceback (most recent call last):\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\base.py\", line 565, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\Tai_Phan_Mem\\python\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2070, in __array__\n    return np.asarray(self._values, dtype=dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'obliteration'\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [None,30,32,35,37,38,39,40],'min_samples_split': [2,150,170,180,190,200]}\n",
    "base_estimator = RandomForestClassifier(random_state=random_state_number)\n",
    "rfc_result = get_performance(param_grid, base_estimator, dataframes)\n",
    "rfc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
